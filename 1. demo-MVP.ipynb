{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo CNN for Wildfire Growth Prediction\n",
    "\n",
    "Below is starter code for a cnn solution to solve the wildfire growth challenge!\n",
    "\n",
    "We provide infrastructure and helper functions to call and process the data.\n",
    "\n",
    "It is up to your team to fill in necessary blanks and improve the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:04:46.432025800Z",
     "start_time": "2024-05-25T15:04:46.423191900Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio as rio\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Paths for data and fires\n",
    "data_path = \"./\"\n",
    "train_path = data_path + \"train/\"\n",
    "test_path = data_path + \"test/\"\n",
    "tr_fnums = [\"fire1209\", \"fire1298\", \"fire1386\", \"fire2034\", \"fire2210\", \"fire2211\", \"fire2212\"]\n",
    "te_fnums = [\"fire2214\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:04:47.056914700Z",
     "start_time": "2024-05-25T15:04:47.039959900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Util variables\n",
    "device = 'cuda'\n",
    "target_shape = (528, 720)\n",
    "\n",
    "# Util functions\n",
    "def pad_to_fit(d, shape):\n",
    "    h, w = d.shape\n",
    "    pad_h = shape[0] - h\n",
    "    pad_w = shape[1] - w\n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "\n",
    "        d = np.pad(d, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant', constant_values=0)\n",
    "    return d\n",
    "\n",
    "def normalize(d):\n",
    "    m = np.mean(d)\n",
    "    s = np.std(d)\n",
    "    return (d - m)/s\n",
    "\n",
    "def tif2np(tif):\n",
    "    with rio.open(tif) as src:\n",
    "        data = src.read(1)  # Read the first band\n",
    "    return pad_to_fit(np.nan_to_num(data, nan=0.0), target_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to load data\n",
    "\n",
    "The load fire function loads and processes data for the denoted fire. The fire is then stacked into a numpy array.\n",
    "\n",
    "The load day function loads in a day of data for a specified fire. \n",
    "\n",
    "<ins>**Additional data should be loaded and specified into this function**.<ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:04:48.750337100Z",
     "start_time": "2024-05-25T15:04:48.736138300Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_day(path, day):\n",
    "    # fire_weather\n",
    "    fwi = path+'/fire_weather/fire_weather_index_day{}.tif'.format(day)\n",
    "    fwi = normalize(tif2np(fwi))\n",
    "    # weather relative humidity\n",
    "    wrh = path+'/weather/noon_relative_humidity_day{}.tif'.format(day)\n",
    "    wrh = normalize(tif2np(wrh))\n",
    "    # weather wind speed\n",
    "    wws = path+'/weather/noon_wind_speed_day{}.tif'.format(day)\n",
    "    wws = normalize(tif2np(wws))\n",
    "    # Add more data here\n",
    "    #...\n",
    "    return [fwi, wrh, wws]\n",
    "\n",
    "def load_fire(fire_num, split = \"Train\"):\n",
    "    path = train_path + fire_num\n",
    "    if split == \"Test\":\n",
    "        path = test_path + fire_num\n",
    "    \n",
    "    ftif = path + \"/fire/{}.tif\".format(fire_num)\n",
    "    if split == \"Test\":\n",
    "        ftif = path + \"/fire/{}_train.tif\".format(fire_num)\n",
    "    fdata = tif2np(ftif)\n",
    "\n",
    "    minjd, maxjd = int(np.min(fdata[np.nonzero(fdata)])), int(np.max(fdata))\n",
    "    lastjd = maxjd\n",
    "    if split == \"Test\":\n",
    "        maxjd += 21\n",
    "    \n",
    "    elev = normalize(tif2np(path+'/topography/dem.tif'))\n",
    "    slope = normalize(tif2np(path+'/topography/slope.tif'))\n",
    "    fuels = tif2np(path+'/fuels/fbp_fuels.tif')\n",
    "    ignition = tif2np(path+'/fire/ignitions.tif')\n",
    "\n",
    "    dataset = []\n",
    "    gt = ignition\n",
    "    cfire = ignition\n",
    "    for d in range(minjd, maxjd):\n",
    "        data = {}\n",
    "\n",
    "        fuels[cfire != 0] = 0\n",
    "        ft = [fuels]\n",
    "        ft.extend([cfire, gt, slope, elev])\n",
    "        ft.extend(load_day(path, d))\n",
    "        ft = np.stack(ft)\n",
    "        data['ft'] = ft\n",
    "\n",
    "        if d < lastjd:\n",
    "            gt = fdata == float(d)\n",
    "            data['gt'] = gt\n",
    "\n",
    "        cfire = np.logical_or(cfire ,gt)\n",
    "        \n",
    "        dataset.append(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the datasets and dataloaders\n",
    "\n",
    "<ins>Create/implement data augmentations/transformations here<ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:05:02.836774300Z",
     "start_time": "2024-05-25T15:04:50.122561700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "class FireDataset(Dataset):\n",
    "    def __init__(self, split=\"Train\"):\n",
    "        fnums = tr_fnums if split==\"Train\" else te_fnums\n",
    "        self.dataset = []\n",
    "        for fnum in fnums:\n",
    "            self.dataset.extend(load_fire(fnum, split=split))\n",
    "        print(len(self.dataset))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "    \n",
    "trainset = FireDataset(split=\"Train\")\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [0.9,0.1])\n",
    "testset = FireDataset(split=\"Test\")\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=8, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network/model\n",
    "\n",
    "In this example, we define a simple 2 layer cnn model. \n",
    "\n",
    "<ins>**Modify the model as you see fit!**<ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:05:16.885520500Z",
     "start_time": "2024-05-25T15:05:16.850073200Z"
    }
   },
   "outputs": [],
   "source": [
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Loss function\n",
    "\n",
    "<ins>**Create/define/specify your own loss function here!**<ins>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:05:18.213276Z",
     "start_time": "2024-05-25T15:05:18.195713Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super(IoULoss, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def forward(self, outputs, labels):\n",
    "        # threshold condition is not differentiable so just use softmaxed data\n",
    "        # Flatten the tensors\n",
    "        outputs = outputs.view(-1)\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        # Compute the intersection\n",
    "        intersection = (outputs * labels).sum()\n",
    "\n",
    "        # Compute the union\n",
    "        union = outputs.sum() + labels.sum() - intersection\n",
    "        iou = intersection / (union + 1e-6)  # Add a small epsilon for numerical stability\n",
    "        loss = 1 - iou\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:05:19.859647600Z",
     "start_time": "2024-05-25T15:05:19.834790900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, jaccard_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:05:24.143361Z",
     "start_time": "2024-05-25T15:05:24.115921900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    total_steps = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        ft = batch['ft'].to(device).float()\n",
    "        gt = batch['gt'].to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(ft).squeeze()\n",
    "\n",
    "        loss = criterion(output, gt)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        total_steps += 1\n",
    "    return running_loss/total_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eval function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:05:26.313256200Z",
     "start_time": "2024-05-25T15:05:26.296623900Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    acc = []\n",
    "    iou = []\n",
    "    f1 = []\n",
    "    total_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            ft = batch['ft'].to(device)\n",
    "            gt = torch.flatten(batch['gt'])\n",
    "\n",
    "            output = torch.flatten(model(ft)).squeeze().cpu()\n",
    "            output = (output > 0.5)\n",
    "\n",
    "            acc.append(accuracy_score(gt, output))\n",
    "            iou.append(jaccard_score(gt, output))\n",
    "            f1.append(f1_score(gt, output))\n",
    "            total_steps += 1\n",
    "    return sum(acc)/total_steps, sum(iou)/total_steps, sum(f1)/total_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference function\n",
    "\n",
    "Saves the inference results to a submission file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:05:27.504780300Z",
     "start_time": "2024-05-25T15:05:27.487818900Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(model, dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        cfire = torch.zeros(target_shape)\n",
    "        for i, day in enumerate(dataloader):\n",
    "            ft = day['ft'].to(device)\n",
    "\n",
    "            # Create the submission file after 10 days\n",
    "            if i > 9:\n",
    "                cfire = torch.logical_or(output, cfire) # define the cumulative fire\n",
    "                ft[0][1] = cfire # set the cumulative fire for the next input\n",
    "                ft[0][2] = output # set the next step fire for the next input\n",
    "            else:\n",
    "                cfire = ft[0][1]\n",
    "\n",
    "            output = model(ft)\n",
    "            output = (output > 0.5)\n",
    "    \n",
    "    # Save the cumulative fire\n",
    "    pred = cfire.cpu().squeeze().numpy()\n",
    "    save_df = pd.DataFrame(pred)  # convert img data to df\n",
    "    save_df.to_csv(\"./output/submission.csv\", index_label='row')\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The training/eval/inference loop\n",
    "\n",
    "<ins>**Define new optimizers here**<ins>\n",
    "\n",
    "<ins>**Utilize a scheduler here**<ins>\n",
    "\n",
    "<ins>**Change the learning rate here**<ins>\n",
    "\n",
    "<ins>**Implement a better early stopping strategy here**<ins>\n",
    "\n",
    "<ins>**Implement other tricks here (i.e. EMA)**<ins>\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available())  \u001B[38;5;66;03m# Should return True\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)  \u001B[38;5;66;03m# Should return the device ID\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mget_device_name(\u001B[38;5;241m0\u001B[39m))  \u001B[38;5;66;03m# Should return the name of your GPU\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Projects\\Wildfire_Hackathon_Complete\\.venv\\lib\\site-packages\\torch\\cuda\\__init__.py:778\u001B[0m, in \u001B[0;36mcurrent_device\u001B[1;34m()\u001B[0m\n\u001B[0;32m    776\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcurrent_device\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[0;32m    777\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Return the index of a currently selected device.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 778\u001B[0m     \u001B[43m_lazy_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_cuda_getDevice()\n",
      "File \u001B[1;32mD:\\Projects\\Wildfire_Hackathon_Complete\\.venv\\lib\\site-packages\\torch\\cuda\\__init__.py:284\u001B[0m, in \u001B[0;36m_lazy_init\u001B[1;34m()\u001B[0m\n\u001B[0;32m    279\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    282\u001B[0m     )\n\u001B[0;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    287\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    288\u001B[0m     )\n",
      "\u001B[1;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.current_device())  # Should return the device ID\n",
    "print(torch.cuda.get_device_name(0))  # Should return the name of your GPU\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-25T15:06:53.298268800Z",
     "start_time": "2024-05-25T15:06:53.251432800Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T15:05:41.728309100Z",
     "start_time": "2024-05-25T15:05:41.600308700Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mCNN1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_features\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      3\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n",
      "Cell \u001B[1;32mIn[19], line 29\u001B[0m, in \u001B[0;36mCNN1.__init__\u001B[1;34m(self, embedding_dim, num_features)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, embedding_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m6\u001B[39m, num_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m):\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28msuper\u001B[39m(CNN1, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[1;32m---> 29\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuelembedding \u001B[38;5;241m=\u001B[39m \u001B[43mFuelEmbeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding_dim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# (266, 433)\u001B[39;00m\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv_block1 \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[0;32m     33\u001B[0m         nn\u001B[38;5;241m.\u001B[39mConv2d(in_channels\u001B[38;5;241m=\u001B[39m(embedding_dim\u001B[38;5;241m+\u001B[39mnum_features\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m), out_channels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m, kernel_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m),\n\u001B[0;32m     34\u001B[0m         nn\u001B[38;5;241m.\u001B[39mBatchNorm2d(num_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     38\u001B[0m         nn\u001B[38;5;241m.\u001B[39mReLU(inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     39\u001B[0m     )\n",
      "Cell \u001B[1;32mIn[19], line 9\u001B[0m, in \u001B[0;36mFuelEmbeddings.__init__\u001B[1;34m(self, embedding_dim)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28msuper\u001B[39m(FuelEmbeddings, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m      8\u001B[0m unique_values \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m13\u001B[39m, \u001B[38;5;241m31\u001B[39m, \u001B[38;5;241m101\u001B[39m, \u001B[38;5;241m425\u001B[39m, \u001B[38;5;241m635\u001B[39m, \u001B[38;5;241m650\u001B[39m, \u001B[38;5;241m665\u001B[39m]\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39munique_values \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43munique_values\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Unique values in the categorical feature\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding_dim \u001B[38;5;241m=\u001B[39m embedding_dim\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mEmbedding(num_embeddings\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(unique_values), embedding_dim\u001B[38;5;241m=\u001B[39membedding_dim)\n",
      "File \u001B[1;32mD:\\Projects\\Wildfire_Hackathon_Complete\\.venv\\lib\\site-packages\\torch\\cuda\\__init__.py:284\u001B[0m, in \u001B[0;36m_lazy_init\u001B[1;34m()\u001B[0m\n\u001B[0;32m    279\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    282\u001B[0m     )\n\u001B[0;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 284\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    285\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    287\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    288\u001B[0m     )\n",
      "\u001B[1;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = CNN1(num_features=8)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = IoULoss()\n",
    "epochs = 100\n",
    "best_miou = 0\n",
    "for e in range(epochs):\n",
    "    loss = train(model, trainloader, optimizer, criterion)\n",
    "    aa, miou, mf1 = eval(model,valloader)\n",
    "\n",
    "    if miou > best_miou:\n",
    "        best_miou = miou\n",
    "        cfire, = inference(model, testloader)\n",
    "        e = str(e)+\"*\"\n",
    "    print(e, \" avg iou loss:{:.3f} avg acc: {:.3f} avg f1: {:.3f} avg iou: {:.3f}\".format(loss, aa, mf1, miou))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Ideas to implement!\n",
    "\n",
    "<ins>**Ensemble learning - voting**<ins>\n",
    "\n",
    "<ins>**Implement hot spot data pipeline**<ins>\n",
    "\n",
    "<ins>**Make better use of temporal data**<ins>\n",
    "\n",
    "<ins>**Get creative!**<ins>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
